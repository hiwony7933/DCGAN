{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./gan_images\"):\n",
    "    os.makedirs(\"./gan_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Sequential()\n",
    "g_model.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2)))\n",
    "g_model.add(BatchNormalization())\n",
    "g_model.add(Reshape((7,7,128)))\n",
    "g_model.add(UpSampling2D())\n",
    "g_model.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "g_model.add(BatchNormalization())\n",
    "g_model.add(Activation(LeakyReLU(0.02)))\n",
    "g_model.add(UpSampling2D())\n",
    "\n",
    "g_model.add(Conv2D(32, kernel_size=5, padding='same'))\n",
    "g_model.add(BatchNormalization())\n",
    "g_model.add(Activation(LeakyReLU(0.02)))\n",
    "\n",
    "g_model.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = Sequential()\n",
    "d_model.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28, 28, 1), padding='same'))\n",
    "d_model.add(Activation(LeakyReLU(0.2)))\n",
    "d_model.add(Dropout(0.3))\n",
    "d_model.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "d_model.add(Activation(LeakyReLU(0.2)))\n",
    "d_model.add(Dropout(0.3))\n",
    "\n",
    "d_model.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "d_model.add(Activation(LeakyReLU(0.2)))\n",
    "d_model.add(Dropout(0.3))\n",
    "\n",
    "d_model.add(Flatten())\n",
    "d_model.add(Dense(1, activation='sigmoid'))\n",
    "d_model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n",
    "d_model.trainable = False\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_input = Input(shape=(100,))\n",
    "dis_output = d_model(g_model(g_input))\n",
    "gan = Model(g_input, dis_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "data_ = X_train.shape[0]\n",
    "rows = X_train.shape[1]\n",
    "cloums = X_train.shape[2]\n",
    "\n",
    "epoch= 1001\n",
    "batch_size = 32\n",
    "saving_interval = 200\n",
    "history= []\n",
    "\n",
    "X_train = X_train.reshape(data_, rows, cloums, 1).astype('float32')\n",
    "#X_train = (X_train - 127.5) / 127.5\n",
    "X_train = X_train / 255.0\n",
    "true = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    start_time=time.time()\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size) # 0~ 60000 범위내에서 32개의 난수 생성\n",
    "    imgs = X_train[idx] # X_train의 idx 32개를 저장\n",
    "    d_loss_real = d_model.train_on_batch(imgs, true) # X_train의 32개는 모두 real 학습\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100)) # 0 ~ 1 범위내에서 (32, 100) 개의 난수 생성\n",
    "    gen_imgs = g_model.predict(noise) # 생성자에 \n",
    "    d_loss_fake = d_model.train_on_batch(gen_imgs, fake) \n",
    "\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    g_loss = gan.train_on_batch(noise, true) \n",
    "\n",
    "    # 기록\n",
    "    record = (epoch, d_loss[0], d_loss[1], g_loss[0], g_loss[1])\n",
    "    history.append(record)\n",
    "\n",
    "    #print('epoch:%d' %i, ' d_Loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss) \n",
    "    \n",
    "    log_mesg = \"%d elapsed : %f \" % (i, (time.time() - start_time))\n",
    "    log_mesg = \"%s: [D loss: %f, acc: %f]\" % (log_mesg, d_loss[0], d_loss[1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
    "    \n",
    "    print(log_mesg)\n",
    "\n",
    "    if i % saving_interval == 0:\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = g_model.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(4, 10, figsize=(15, 5))\n",
    "        count = 0\n",
    "\n",
    "        print(y_train[idx[:10]])\n",
    "\n",
    "        for j in range(axs.shape[1]) :\n",
    "            \n",
    "            axs[0, j].axis('off')\n",
    "            axs[0, j].imshow(gen_imgs[j].reshape((rows, cloums)), cmap='gray') \n",
    "            axs[1, j].axis('off')\n",
    "            axs[1, j].imshow(imgs[j].reshape((rows, cloums)), cmap='gray')    \n",
    "            axs[2, j].axis('off')\n",
    "            axs[2, j].set_ylim([0, 1])\n",
    "            axs[2, j].set_xlim([0, 1]) \n",
    "            axs[2, j].scatter(gen_imgs[j], gen_imgs[j], color='b')\n",
    "            axs[3, j].axis('off')\n",
    "            axs[3, j].set_ylim([0, 1])\n",
    "            axs[3, j].set_xlim([0, 1])\n",
    "            axs[3, j].scatter(imgs[j], imgs[j], color='r')\n",
    "            \n",
    "        fig.savefig(\"gan_images/gan_mnist_%d.png\" % i)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './Gan_model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "g_model.save_weights('my_g_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd \n",
    "\n",
    "df = DataFrame(history, columns=['epoch', 'd_loss', 'd_acc', 'g_loss', 'g_acc'])\n",
    "df.plot(y=['d_loss', 'g_loss'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.ones((10, 1))\n",
    "fake = np.zeros((10, 1))\n",
    "\n",
    "np.random.seed(0)\n",
    "#test\n",
    "idx = np.random.randint(0, 32, 10) # 0~ 60000 범위내에서 32개의 난수 생성\n",
    "\n",
    "imgs = X_train[idx] # X_train의 idx 32개를 저장\n",
    "d_loss_real = d_model.train_on_batch(imgs, true) # X_train의 32개는 모두 real 학습\n",
    "print(y_train[idx])\n",
    "\n",
    "noise = np.random.normal(0, 1, (10, 100)) # 0 ~ 1 범위내에서 (32, 100) 개의 난수 생성\n",
    "gen_imgs = g_model.predict(noise) # 생성자에 \n",
    "d_loss_fake = d_model.train_on_batch(gen_imgs, fake) \n",
    "\n",
    "d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "g_loss = gan.train_on_batch(noise, true) \n",
    "\n",
    "# 기록\n",
    "record = (epoch, d_loss[0], d_loss[1], g_loss[0], g_loss[1])\n",
    "history.append(record)\n",
    "\n",
    "log_mesg = \"%d: [D loss: %f, acc: %f]\" % (1, d_loss[0], d_loss[1])\n",
    "log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
    "\n",
    "print(log_mesg)\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 1, (25, 100))\n",
    "gen_imgs = g_model.predict(noise)\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "fig, axs = plt.subplots(4, 10, figsize=(15, 5))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for j in range(axs.shape[1]) :\n",
    "    axs[0, j].axis('off')\n",
    "    axs[0, j].imshow(gen_imgs[j].reshape((rows, cloums)), cmap='gray') \n",
    "    axs[1, j].axis('off')\n",
    "    axs[1, j].imshow(imgs[j].reshape((rows, cloums)), cmap='gray')    \n",
    "    axs[2, j].axis('off')\n",
    "    axs[2, j].set_ylim([0, 1])\n",
    "    axs[2, j].set_xlim([0, 1]) \n",
    "    axs[2, j].scatter(gen_imgs[j], gen_imgs[j], color='b')\n",
    "    axs[3, j].axis('off')\n",
    "    axs[3, j].set_ylim([0, 1])\n",
    "    axs[3, j].set_xlim([0, 1])\n",
    "    axs[3, j].scatter(imgs[j], imgs[j], color='r')\n",
    "    \n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gan_train(epoch, batch_size, saving_interval) :\n",
    "#     (X_train, _), (_,_) = mnist.load_data()\n",
    "#     X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "#     X_train = (X_train -127.5) / 127.5\n",
    "#     true = np.ones((batch_size, 1))\n",
    "#     fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "#     for i in range(epoch) :\n",
    "#         idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "#         imgs = X_train[idx]\n",
    "#         d_loss_real = d_model.train_on_batch(imgs, true)\n",
    "        \n",
    "#         noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "#         gen_imgs = g_model.predict(noise)\n",
    "#         d_loss_fake = d_model.train_on_batch(gen_imgs, fake) \n",
    "        \n",
    "#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "#         g_loss = gan.train_on_batch(noise, true) \n",
    "        \n",
    "#         print('epoch:%d' %i, ' dLoss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss) \n",
    "        \n",
    "#         if i % saving_interval == 0:\n",
    "#             #r, c = 5, 5\n",
    "#             noise = np.random.normal(0, 1, (25, 100))\n",
    "#             gen_imgs = g_model.predict(noise)\n",
    "\n",
    "#             # Rescale images 0 - 1\n",
    "#             gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "#             fig, axs = plt.subplots(5, 5)\n",
    "#             count = 0\n",
    "#             for j in range(5):\n",
    "#               for k in range(5):\n",
    "#                   axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
    "#                   axs[j, k].axis('off')\n",
    "#                   count += 1\n",
    "#             fig.savefig(\"gan_images/gan_mnist_%d.png\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}