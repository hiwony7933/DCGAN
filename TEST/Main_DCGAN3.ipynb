{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.models import * \n",
    "from keras.layers import * \n",
    "from keras.optimizers import * \n",
    "from keras.datasets import mnist \n",
    "import keras.backend as K \n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "K.set_image_data_format('channels_last') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gan: \n",
    "    def __init__(self, img_data): \n",
    "        img_size = img_data.shape[1] \n",
    "        channel = img_data.shape[3] if len(img_data.shape) >= 4 else 1 \n",
    "        self.img_data = img_data \n",
    "        self.input_shape = (img_size, img_size, channel) \n",
    "        self.img_rows = img_size \n",
    "        self.img_cols = img_size \n",
    "        self.channel = channel \n",
    "        self.noise_size = 100 \n",
    "        \n",
    "        self.create_d() \n",
    "        self.create_g() \n",
    "        \n",
    "        optimizer = Adam(lr=0.0008) \n",
    "        self.D.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "        \n",
    "        optimizer = Adam(lr=0.0004) \n",
    "        self.D.trainable = False \n",
    "        self.AM = Sequential() \n",
    "        self.AM.add(self.G) \n",
    "        self.AM.add(self.D) \n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer) \n",
    "        print('AM')\n",
    "        #self.AM.summary()\n",
    "        \n",
    "    def create_d(self): \n",
    "        self.D = Sequential() \n",
    "        depth = 64 \n",
    "        dropout = 0.4 \n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=self.input_shape, padding='same')) \n",
    "        self.D.add(LeakyReLU(alpha=0.2)) \n",
    "        self.D.add(Dropout(dropout)) \n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same')) \n",
    "        self.D.add(LeakyReLU(alpha=0.2)) \n",
    "        self.D.add(Dropout(dropout)) \n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same')) \n",
    "        self.D.add(LeakyReLU(alpha=0.2)) \n",
    "        self.D.add(Dropout(dropout)) \n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same')) \n",
    "        self.D.add(LeakyReLU(alpha=0.2)) \n",
    "        self.D.add(Dropout(dropout)) \n",
    "        self.D.add(Flatten()) \n",
    "        self.D.add(Dense(1)) \n",
    "        self.D.add(Activation('sigmoid')) \n",
    "        print('Discriminator')\n",
    "        #self.D.summary() \n",
    "        return self.D \n",
    "        \n",
    "    def create_g(self): \n",
    "        self.G = Sequential() \n",
    "        dropout = 0.4 \n",
    "        depth = 64+64+64+64 \n",
    "        dim = 7 \n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=self.noise_size)) \n",
    "        self.G.add(BatchNormalization(momentum=0.9)) \n",
    "        self.G.add(Activation('relu')) \n",
    "        self.G.add(Reshape((dim, dim, depth))) \n",
    "        self.G.add(Dropout(dropout)) \n",
    "        self.G.add(UpSampling2D()) \n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same')) \n",
    "        self.G.add(BatchNormalization(momentum=0.9)) \n",
    "        self.G.add(Activation('relu')) \n",
    "        self.G.add(UpSampling2D()) \n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same')) \n",
    "        self.G.add(BatchNormalization(momentum=0.9)) \n",
    "        self.G.add(Activation('relu')) \n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same')) \n",
    "        self.G.add(BatchNormalization(momentum=0.9)) \n",
    "        self.G.add(Activation('relu')) \n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same')) \n",
    "        self.G.add(Activation('sigmoid')) \n",
    "        print('Generator')\n",
    "        #self.G.summary() \n",
    "        return self.G \n",
    "        \n",
    "    def train(self, batch_size=100): \n",
    "        images_train = self.img_data[np.random.randint(0, self.img_data.shape[0], size=batch_size), :, :, :] \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_size]) \n",
    "        images_fake = self.G.predict(noise) \n",
    " \n",
    "        x = np.concatenate((images_train, images_fake)) \n",
    "        y = np.ones([2*batch_size, 1]) \n",
    "        y[batch_size:, :] = 0 \n",
    "        self.D.trainable = True \n",
    "        d_loss = self.D.train_on_batch(x, y) \n",
    "        decision = self.D.predict(images_fake)\n",
    "\n",
    "        y = np.ones([batch_size, 1]) \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_size]) \n",
    "        self.D.trainable = False \n",
    "        a_loss = self.AM.train_on_batch(noise, y) \n",
    "\n",
    "        return d_loss, a_loss, images_fake, decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60000, 28, 28, 1)\nDiscriminator\nGenerator\nAM\n"
    }
   ],
   "source": [
    "#x_train, x_test, y_train, y_test = np.load('binary_image_data.npy')\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1).astype('float32')\n",
    "x_train = (x_train -127.5) / 127.5 \n",
    "print(x_train.shape) \n",
    "\n",
    "# Init network \n",
    "gan = Gan(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "맨 처음 확인 \n",
    "# generator = gan.create_g()\n",
    "# noise = np.random.uniform(-1.0, 1.0, size=[1,100])\n",
    "# generated_image = generator.predict(noise)\n",
    "# plt.imshow(generated_image.reshape((28,28)), cmap='gray')\n",
    "\n",
    "# discriminator = gan.create_d()\n",
    "# decision = discriminator.predict(generated_image)\n",
    "# print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "600\n"
    }
   ],
   "source": [
    "# Some parameters. \n",
    "epochs = 2\n",
    "sample_size = 10 \n",
    "batch_size = 100\n",
    "train_per_epoch = x_train.shape[0] // batch_size\n",
    "history=[]\n",
    "print(train_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n[0.]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7aecad32423d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mttt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_decisi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtt_decisi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_decisi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mttt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_d_loss = 0.0 \n",
    "total_a_loss = 0.0\n",
    "total_decision = []\n",
    "\n",
    "d_loss, a_loss, imgs, decision = gan.train(batch_size) \n",
    "\n",
    "t_decisi = 0.0\n",
    "for i in range(len(decision)) :\n",
    "    if decision[i] > 0 :\n",
    "        t_decisi += decision[i] \n",
    "        #print(decision[i])\n",
    "        #plt.imshow(imgs[i].reshape((gan.img_rows, gan.img_cols)))\n",
    "        #plt.show()\n",
    "    else :    \n",
    "        print(decision[i])\n",
    "\n",
    "ttt = len(t_decisi)\n",
    "print(ttt)\n",
    "tt_decisi = t_decisi / ttt\n",
    "print(tt_decisi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Discriminator\n[0 0 0 0 0 0 0 0 0 0]\n(10, 1)\n[[0.4978253 ]\n [0.49781877]\n [0.49782714]\n [0.49776918]\n [0.49781188]\n [0.49780712]\n [0.497805  ]\n [0.4977731 ]\n [0.49778843]\n [0.49783185]]\n정답률(Accuracy): tf.Tensor(\n[[[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n ...\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]\n\n\n [[[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  ...\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]\n\n  [[0.]\n   [0.]\n   [0.]\n   ...\n   [0.]\n   [0.]\n   [0.]]]], shape=(10, 28, 28, 1), dtype=float32)\n"
    }
   ],
   "source": [
    "disei = gan.create_d()\n",
    "y_= disei.predict(imgs[:10])\n",
    "predicted = np.argmax(y_, axis=1)\n",
    "print(predicted)\n",
    "print(y_.shape)\n",
    "print(y_)\n",
    "\n",
    "#print(disei.evaluate(test_x, y_)[1])\n",
    "print('정답률(Accuracy):', metrics.accuracy(test_x[:10], y_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = x_test[ : 1000]\n",
    "test_x = test_x.reshape((test_x.shape[0],) + (28, 28, 1)).astype('float32') \n",
    "test_x = test_x / 255.0 \n",
    "\n",
    "print(test_x.shape) \n",
    "plt.imshow(test_x[1].reshape((28,28)))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "# Some parameters. \n",
    "epochs = 2\n",
    "sample_size = 10 \n",
    "batch_size = 100\n",
    "train_per_epoch = x_train.shape[0] // batch_size\n",
    "history=[]\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs): \n",
    "    start_time = time.time()\n",
    "    total_d_loss = 0.0 \n",
    "    total_a_loss = 0.0\n",
    "    \n",
    "    for batch in range(0, train_per_epoch): \n",
    "        d_loss, a_loss, imgs = gan.train(batch_size) \n",
    "        total_d_loss += d_loss \n",
    "        total_a_loss += a_loss \n",
    "        \n",
    "    total_d_loss /= train_per_epoch \n",
    "    total_a_loss /= train_per_epoch\n",
    "    \n",
    "    recode =(epoch, total_d_loss, total_a_loss)\n",
    "    history.append(recode)\n",
    "\n",
    "    print(\"Epoch : {}, elapsed : {}, D Loss: {}, AM Loss: {}\".format(epoch + 1, time.time() - start_time, total_d_loss, total_a_loss)) \n",
    "    \n",
    "    fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1)) \n",
    "    \n",
    "    for i in range(0, sample_size): \n",
    "        ax[i].set_axis_off() \n",
    "        ax[i].imshow(imgs[i].reshape((gan.img_rows, gan.img_cols)), interpolation='nearest')\n",
    "    plt.show() \n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame \n",
    "\n",
    "df = DataFrame(history, columns=['epoch', 'total_d_loss', 'total_a_loss'])\n",
    "\n",
    "df.plot(y=['d_loss', 'a_loss'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}